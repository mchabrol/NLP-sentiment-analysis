{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewTokenizer:\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        \"\"\"Basic text cleaning and tokenization\"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "\n",
    "class Word2VecEmbedder:\n",
    "    def __init__(self, vector_size: int=100, window:int =5, min_count:int =2, sg:int =1, workers:int =4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, tokenized_reviews: List[str]) -> None:\n",
    "        \"\"\"Train the Word2Vec model on tokenized reviews\n",
    "        Args \n",
    "            tokenized_reviews: reviews cleaned and tokenized with ReviewTokenizer\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Word2Vec model...\")\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokenized_reviews,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=self.sg,\n",
    "            workers=self.workers\n",
    "        )\n",
    "        logger.info(\"Word2Vec model trained successfully\")\n",
    "\n",
    "    def embed_reviews(self, tokenized_reviews: List[str]) -> np.ndarray:\n",
    "        \"\"\"Compute embeddings by averaging word vectors for each review\n",
    "        Args \n",
    "            tokenized_reviews: List of reviews, each review is a list of tokenized words\n",
    "        Returns\n",
    "            np.ndarray: 2D array containing embeddings for each review\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for tokens in tokenized_reviews:\n",
    "            vectors = [self.model.wv[token] for token in tokens if token in self.model.wv]\n",
    "            if vectors:\n",
    "                embed = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                embed = np.zeros(self.vector_size)\n",
    "            embeddings.append(embed)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def save_embeddings(self, embeddings: np.ndarray, filepath: str) -> None:\n",
    "        \"\"\"Saves the embeddings obtained with word2vec to a given filepath\n",
    "        Args \n",
    "            embeddings: Array containing the embeddings.\n",
    "            filepath: Path where embeddings will be saved\n",
    "        \"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        logger.info(f\"Embeddings saved successfully to {filepath}\")\n",
    "\n",
    "    def load_embeddings(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"Retrieves embeddings obtained with word2vec\n",
    "        Args\n",
    "            filepath: Path from where embeddings will be loaded\n",
    "        Returns \n",
    "            np.ndarray: Loaded embeddings array\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, classifier=None):\n",
    "        self.classifier = classifier if classifier else LogisticRegression(max_iter=500)\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Fit the model to train data\n",
    "        Args\n",
    "            X: Feature \n",
    "            y: True labels \n",
    "        \"\"\"\n",
    "        self.classifier.fit(X, y)\n",
    "        logger.info(\"Classifier trained successfully.\")\n",
    "\n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, str]:\n",
    "        \"\"\"Evaluate the model on the provided dataset\n",
    "        Args\n",
    "            X: Feature \n",
    "            y: True labels \n",
    "        \n",
    "        Returns\n",
    "            Tuple[float, str]: Accuracy score and detailed classification report\n",
    "        \"\"\"\n",
    "        predictions = self.classifier.predict(X)\n",
    "        accuracy = accuracy_score(y, predictions)\n",
    "        report = classification_report(y, predictions)\n",
    "        return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('aclImdb/df_train.csv')\n",
    "df_test = pd.read_csv('aclImdb/df_test.csv')\n",
    "\n",
    "# tokenisation\n",
    "tokenized_reviews_train = [ReviewTokenizer.tokenize(text) for text in df_train['comment']]\n",
    "\n",
    "# train embeddings\n",
    "embedder = Word2VecEmbedder()\n",
    "embedder.train(tokenized_reviews_train)\n",
    "X_embeddings = embedder.embed_reviews(tokenized_reviews_train)\n",
    "embedder.save_embeddings(X_embeddings, 'aclImdb/embeddings/X_train_word2vec_embeddings.pkl')\n",
    "\n",
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_embeddings, df_train['sentiment'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# train and evaluate classifier\n",
    "clf = SentimentClassifier(classifier=LinearSVC())\n",
    "clf.train(X_embeddings, df_train['sentiment'])\n",
    "\n",
    "train_accuracy, train_report = clf.evaluate(X_embeddings, df_train['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Accuracy on train set for Word2vec and SVC ========\n",
      " 0.86836\n",
      "======== Accuracy on test set for Word2vec and SVC ========\n",
      " 0.85576\n"
     ]
    }
   ],
   "source": [
    "#Check performance on test set \n",
    "tokenized_reviews_test = [ReviewTokenizer.tokenize(text) for text in df_test['comment']]\n",
    "\n",
    "X_test_embeddings = embedder.embed_reviews(tokenized_reviews_test)\n",
    "test_accuracy, test_report = clf.evaluate(X_test_embeddings, df_test['sentiment'])\n",
    "\n",
    "print(\"======== Accuracy on train set for Word2vec and SVC ========\\n\", train_accuracy)\n",
    "print(\"======== Accuracy on test set for Word2vec and SVC ========\\n\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
